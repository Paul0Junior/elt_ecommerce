





# Importando bibliotecas para manipulação do arquivo:
import zipfile as zp
import os
import pandas as pd
import numpy as np
from google.cloud import bigquery





origem_zip = 'D:/Estudos/Projetos Pessoais/elt_ecommerce/dataset/archive.zip'
destino_zip = 'D:/Estudos/Projetos Pessoais/elt_ecommerce/dataset/'
dataset = 'D:/Estudos/Projetos Pessoais/elt_ecommerce/dataset/data.csv'
global dataset


# Criando o diretório caso não exista ainda:
os.makedirs(destino_zip, exist_ok=True)





with zp.ZipFile(origem_zip, 'r') as zip_ref:
    zip_ref.extractall(destino_zip)

print(f'Arquivos extraídos para {destino_zip}')





import chardet

# Function to detect encoding of a file
def detect_encoding(file_path):
    with open(file_path, 'rb') as file:
        # Read a portion of the file, as full file reading might be unnecessary and slow
        raw_data = file.read(100000)  # Read the first 100,000 bytes
        result = chardet.detect(raw_data)
        encoding = result['encoding']
        confidence = result['confidence']
    return encoding, confidence

# Path to your file
file_path = dataset

# Detect encoding
encoding, confidence = detect_encoding(file_path)
print(f"Detected encoding: {encoding} with confidence: {confidence}")





df = pd.read_csv(dataset, encoding='ISO-8859-1')
print(df.head())





import os
os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = "D:/Estudos/Projetos Pessoais/elt_ecommerce/acces_key_gcp/key.json"





def tabela_existe(client, dataset_id, table_id):
    try:
        client.get_table(f'{dataset_id}.{table_id}')
        return True
    except Exception as e:
        if 'Not found' in str(e):
            return False
        else:
            raise

def truncar_tabela(client, dataset_id, table_id):
    query = f'TRUNCATE TABLE `{dataset_id}.{table_id}`'
    query_job = client.query(query)
    query_job.result() # Aguarda o fim da execução
    print(f' A tabela {dataset_id}.{table_id} foi truncada')

def main():
    client = bigquery.Client()
    dataset_id = 'ecomerce'
    table_id = 'ecomerce'

    if tabela_existe(client, dataset_id, table_id):
        truncar_tabela(client, dataset_id, table_id)
    else:
        print(f'A tabela {dataset_id}.{table_id} não existe')

if __name__ == '__main__':
    main()





#Instalada a biblioteca google cloud bigquery
#pip install google-cloud-bigquery


from google.cloud import bigquery

# Parâmetros
dataset_id = 'ecomerce'
table_id = 'ecomerce'
source_file_name = 'D:/Estudos/Projetos Pessoais/elt_ecommerce/dataset/data.csv'

def carga_gbq(dataset_id, table_id, source_file_name):
    #Cria um cliente BigQuery
    client = bigquery.Client()

    #Define o dataset e a tabela
    table_ref = client.dataset(dataset_id).table(table_id)

    #Configura o job de carregamento
    job_config = bigquery.LoadJobConfig(
        source_format=bigquery.SourceFormat.CSV,
        skip_leading_rows=1, #Pular a primeira linha(cabeçalho)
        autodetect=True,     #Tentar detectar o esquema automáticamente(Colunas)
    )

    #Lê o arquivo CSV e inicia o job de carregamento
    with open(source_file_name, "rb") as source_file:
        load_job = client.load_table_from_file(source_file, table_ref, job_config=job_config)

    print('Iniciando o job de carregamento {}'.format(load_job.job_id))

    load_job.result() #Espera o job ser concluído

    print('Carregamento concluído.')

    #Verifica o número de linhas carregadas
    destination_table = client.get_table(table_ref)
    print('Carregou {} linhas.'.format(destination_table.num_rows))





#Executa a carga
carga_gbq(dataset_id, table_id, source_file_name)



